{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T22:39:58.146774Z",
     "start_time": "2019-10-07T22:39:55.588338Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# get data PATH from current directory\n",
    "PATH = os.path.join(Path(os.getcwd()).parent, 'data')\n",
    "\n",
    "\n",
    "PATH_VOCAB = os.path.join(PATH, 'vocabularies')\n",
    "PATH_SUB = os.path.join(PATH, 'submissions')\n",
    "PATH_MODEL = os.path.join(PATH, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T22:40:00.977438Z",
     "start_time": "2019-10-07T22:40:00.939273Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open(os.path.join(PATH, 'classes_dict.pkl'), 'rb')\n",
    "classes_dict = pickle.load(file)\n",
    "\n",
    "cat_dict = classes_dict['class_stoi']\n",
    "cat_dict_inv = classes_dict['class_itos']\n",
    "dict_count = classes_dict['class_count']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T22:40:02.893658Z",
     "start_time": "2019-10-07T22:40:02.301243Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(os.path.join(PATH, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T22:40:06.957135Z",
     "start_time": "2019-10-07T22:40:04.092571Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "\n",
    "# Leo vocabulario\n",
    "\n",
    "file = open(os.path.join(PATH_VOCAB, 'vocab_test_mytk.pkl'), 'rb')\n",
    "vocab = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T22:40:09.743994Z",
     "start_time": "2019-10-07T22:40:09.532044Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "file = open( os.path.join(PATH_VOCAB, 'vocab_test_red.pkl'), 'rb')\n",
    "vocab_mix = pickle.load(file)\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                             strip_accents='unicode',\n",
    "                             min_df=2\n",
    "                            )\n",
    "\n",
    "tokenizer = vectorizer.build_analyzer()\n",
    "\n",
    "\n",
    "counter = Counter(vocab_mix.values())\n",
    "\n",
    "a = [x for x in counter if counter[x]>1]\n",
    "\n",
    "\n",
    "\n",
    "vocab_transf = {}\n",
    "for key in vocab_mix:\n",
    "    vocab_transf[vocab_mix[key]] = 'r_{}'.format(key)\n",
    "    \n",
    "vocab_transf\n",
    "\n",
    "\n",
    "def my_tokenizer(string):\n",
    "    tokens = tokenizer(string)\n",
    "    list_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in vocab_mix:\n",
    "            num = vocab_mix[token]\n",
    "            #print(token, vocab_mix[token], counter[num])\n",
    "            if counter[num]>1:\n",
    "                list_tokens.append(vocab_transf[vocab_mix[token]])\n",
    "            list_tokens.append(token)\n",
    "\n",
    "    #print(list_tokens, string)\n",
    "    return list_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T22:40:17.171216Z",
     "start_time": "2019-10-07T22:40:12.918696Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "                                analyzer = 'word',\n",
    "                                strip_accents='unicode',\n",
    "                                vocabulary = vocab,\n",
    "                                tokenizer=my_tokenizer\n",
    "                                )\n",
    "\n",
    "\n",
    "vectors = vectorizer.fit_transform(X_test.title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T22:40:17.177855Z",
     "start_time": "2019-10-07T22:40:17.172317Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "import numpy as np\n",
    "\n",
    "def csr_to_tensor(X_sample):\n",
    "    coo = coo_matrix(X_sample)\n",
    "\n",
    "    values = coo.data\n",
    "    indices = np.vstack((coo.row, coo.col))\n",
    "\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    shape = coo.shape\n",
    "\n",
    "    return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "\n",
    "def predict(model , X_test):\n",
    "    images = csr_to_tensor(X_test)\n",
    "    model2 = model.to('cpu')\n",
    "    outputs = model2(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    return predicted.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model, make predictions and make submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that predictions are made using CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-07T22:52:52.190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003 3\n",
      "0.0003 4\n",
      "0.0003 5\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "vec_lr = [0.0003]\n",
    "vec_v = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "for lr, v in product(vec_lr, vec_v):\n",
    "    \n",
    "    print(lr, v)\n",
    "    # load each model\n",
    "    model = torch.load(os.path.join(PATH_MODEL, 'model_test_mytk_prop20_lr{}_v{}.pt'.format(lr, v)))\n",
    "    \n",
    "    # predict\n",
    "    y_pred = predict(model.eval(), vectors)\n",
    "    \n",
    "    \n",
    "    list_y_pred = []\n",
    "    for y in y_pred:\n",
    "        list_y_pred.append(cat_dict_inv[y])\n",
    "\n",
    "    \n",
    "    \n",
    "    # Make submission file\n",
    "    X_test['category'] = list_y_pred    \n",
    "    X = X_test.drop(columns=['title', 'language'])\n",
    "    X = X.set_index('id')\n",
    "    X.to_csv(os.path.join(PATH_SUB, 'sub_test_mytk_prop20_lr{}_v{}.csv'.format(lr, v)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Vote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_last_mytk_03_v1 = pd.read_csv(os.path.join(PATH_SUB,'sub_test_last_mytk_prop20_lr0.0003_v1.csv'))\n",
    "sub_last_mytk_03_v2 = pd.read_csv(os.path.join(PATH_SUB,'sub_test_last_mytk_prop20_lr0.0003_v2.csv'))\n",
    "sub_last_mytk_03_v3 = pd.read_csv(os.path.join(PATH_SUB,'sub_test_last_mytk_prop20_lr0.0003_v3.csv'))\n",
    "sub_last_mytk_03_v4 = pd.read_csv(os.path.join(PATH_SUB,'sub_test_last_mytk_prop20_lr0.0003_v4.csv'))\n",
    "sub_last_mytk_03_v5 = pd.read_csv(os.path.join(PATH_SUB,'sub_test_last_mytk_prop20_lr0.0003_v5.csv'))\n",
    "sub_last_mytk_03_v6 = pd.read_csv(os.path.join(PATH_SUB,'sub_test_last_mytk_prop20_lr0.0003_v6.csv'))\n",
    "sub_last_mytk_03_v7 = pd.read_csv(os.path.join(PATH_SUB,'sub_test_last_mytk_prop20_lr0.0003_v7.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "\n",
    "list_counters = []\n",
    "# best 2\n",
    "models1 = [sub_last_mytk_03_v1.category, sub_last_mytk_03_v2.category,sub_last_mytk_03_v3.category,\n",
    "          sub_last_mytk_03_v4.category, sub_last_mytk_03_v5.category, sub_last_mytk_03_v6.category,\n",
    "          sub_last_mytk_03_v7.category]\n",
    "\n",
    "\n",
    "for i in range(len(sub08_03.category)):\n",
    "    cnt = Counter()\n",
    "    for model in models1:\n",
    "        cnt[model[i]] += 1\n",
    "    \n",
    "    list_counters.append(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate submission file of this group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "ide = []\n",
    "for i,count in enumerate(list_counters):\n",
    "   # print(max(count))\n",
    "    categories.append(max(count, key=count.get))\n",
    "    ide.append(i)\n",
    "\n",
    "sub_mix = pd.DataFrame(categories, columns = ['category']) #, index=['id])', 'b', 'c']) \n",
    "sub_mix['id'] = ide\n",
    "sub_mix = sub_mix.set_index('id')\n",
    "\n",
    "\n",
    "sub_mix.to_csv(os.path.join(PATH_SUB,'sub_mytk_20.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs231n",
   "language": "python",
   "name": "cs231n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
