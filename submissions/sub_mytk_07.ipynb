{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T19:21:43.889286Z",
     "start_time": "2019-10-07T19:21:43.883853Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# get data PATH from current directory\n",
    "PATH = os.path.join(Path(os.getcwd()).parent, 'data')\n",
    "\n",
    "\n",
    "PATH_VOCAB = os.path.join(PATH, 'vocabularies')\n",
    "PATH_SUB = os.path.join(PATH, 'submissions')\n",
    "PATH_MODEL = os.path.join(PATH, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T19:21:44.240675Z",
     "start_time": "2019-10-07T19:21:44.232890Z"
    }
   },
   "outputs": [],
   "source": [
    "file = open(os.path.join(PATH, 'classes_dict.pkl'), 'rb')\n",
    "classes_dict = pickle.load(file)\n",
    "\n",
    "cat_dict = classes_dict['class_stoi']\n",
    "cat_dict_inv = classes_dict['class_itos']\n",
    "dict_count = classes_dict['class_count']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T19:22:19.344126Z",
     "start_time": "2019-10-07T19:22:18.710360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Kit Maternidade Bolsa-mala Baby/bebe Vinho Men...</td>\n",
       "      <td>portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Trocador De Fraldas Fisher Price Feminino Rosa...</td>\n",
       "      <td>portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Motor Ventoinha - Fiat Idea / Palio 1.8 - A 04...</td>\n",
       "      <td>portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Amortecedor Mola Batente D Dir New Civic 14 - ...</td>\n",
       "      <td>portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Cadeirinha De Carro Bebê Princesa Princess 9 A...</td>\n",
       "      <td>portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246950</td>\n",
       "      <td>246950</td>\n",
       "      <td>Disco Freno Delantero Ford Escort 88/94 Nuevo</td>\n",
       "      <td>spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246951</td>\n",
       "      <td>246951</td>\n",
       "      <td>Radio Comunicador Walk Talk Baofeng 777s Profi...</td>\n",
       "      <td>portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246952</td>\n",
       "      <td>246952</td>\n",
       "      <td>Calculadora De Escritorio Grande 150$</td>\n",
       "      <td>spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246953</td>\n",
       "      <td>246953</td>\n",
       "      <td>Conj Mesa P/ Sala De Jantar C/ 06 Cadeiras Ams...</td>\n",
       "      <td>portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246954</td>\n",
       "      <td>246954</td>\n",
       "      <td>Cesto Residuos Tacho Basura Automatico 30l + 1...</td>\n",
       "      <td>spanish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>246955 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title    language\n",
       "0            0  Kit Maternidade Bolsa-mala Baby/bebe Vinho Men...  portuguese\n",
       "1            1  Trocador De Fraldas Fisher Price Feminino Rosa...  portuguese\n",
       "2            2  Motor Ventoinha - Fiat Idea / Palio 1.8 - A 04...  portuguese\n",
       "3            3  Amortecedor Mola Batente D Dir New Civic 14 - ...  portuguese\n",
       "4            4  Cadeirinha De Carro Bebê Princesa Princess 9 A...  portuguese\n",
       "...        ...                                                ...         ...\n",
       "246950  246950      Disco Freno Delantero Ford Escort 88/94 Nuevo     spanish\n",
       "246951  246951  Radio Comunicador Walk Talk Baofeng 777s Profi...  portuguese\n",
       "246952  246952              Calculadora De Escritorio Grande 150$     spanish\n",
       "246953  246953  Conj Mesa P/ Sala De Jantar C/ 06 Cadeiras Ams...  portuguese\n",
       "246954  246954  Cesto Residuos Tacho Basura Automatico 30l + 1...     spanish\n",
       "\n",
       "[246955 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = pd.read_csv(os.path.join(PATH, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T19:23:15.507419Z",
     "start_time": "2019-10-07T19:23:15.445899Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "\n",
    "# Leo vocabulario\n",
    "\n",
    "file = open(os.path.join(PATH_VOCAB, 'vocab_07_test_mytk.pkl'), 'rb')\n",
    "vocab = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T19:24:40.146246Z",
     "start_time": "2019-10-07T19:24:39.916227Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "file = open( os.path.join(PATH_VOCAB, 'vocab_test_red.pkl'), 'rb')\n",
    "vocab_mix = pickle.load(file)\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                             strip_accents='unicode',\n",
    "                             min_df=2\n",
    "                            )\n",
    "\n",
    "tokenizer = vectorizer.build_analyzer()\n",
    "\n",
    "\n",
    "counter = Counter(vocab_mix.values())\n",
    "\n",
    "a = [x for x in counter if counter[x]>1]\n",
    "\n",
    "\n",
    "\n",
    "vocab_transf = {}\n",
    "for key in vocab_mix:\n",
    "    vocab_transf[vocab_mix[key]] = 'r_{}'.format(key)\n",
    "    \n",
    "vocab_transf\n",
    "\n",
    "\n",
    "def my_tokenizer(string):\n",
    "    tokens = tokenizer(string)\n",
    "    list_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in vocab_mix:\n",
    "            num = vocab_mix[token]\n",
    "            #print(token, vocab_mix[token], counter[num])\n",
    "            if counter[num]>1:\n",
    "                list_tokens.append(vocab_transf[vocab_mix[token]])\n",
    "            list_tokens.append(token)\n",
    "\n",
    "    #print(list_tokens, string)\n",
    "    return list_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T19:24:59.411843Z",
     "start_time": "2019-10-07T19:24:55.373250Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "                                analyzer = 'word',\n",
    "                                strip_accents='unicode',\n",
    "                                vocabulary = vocab,\n",
    "                                tokenizer=my_tokenizer\n",
    "                                )\n",
    "\n",
    "\n",
    "vectors = vectorizer.fit_transform(X_test.title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T19:25:23.412312Z",
     "start_time": "2019-10-07T19:25:23.404054Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "import numpy as np\n",
    "\n",
    "def csr_to_tensor(X_sample):\n",
    "    coo = coo_matrix(X_sample)\n",
    "\n",
    "    values = coo.data\n",
    "    indices = np.vstack((coo.row, coo.col))\n",
    "\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    shape = coo.shape\n",
    "\n",
    "    return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "\n",
    "def predict(model , X_test):\n",
    "    images = csr_to_tensor(X_test)\n",
    "    model2 = model.to('cpu')\n",
    "    outputs = model2(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    return predicted.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model, make predictions and make submission file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that predictions are made using CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "vec_lr = [0.0006, 0.0005]\n",
    "vec_v = [1, 2, 3, 4, 5]\n",
    "\n",
    "for lr, v in product(vec_lr, vec_v):\n",
    "    \n",
    "    # load each model\n",
    "    model = torch.load(os.path.join(PATH_MODEL, 'model_test07_mytk_prop10_lr{}_v{}.pt'.format(lr, v)))\n",
    "    \n",
    "    # predict\n",
    "    y_pred = predict(model.eval(), vectors)\n",
    "    \n",
    "    \n",
    "    list_y_pred = []\n",
    "    for y in y_pred:\n",
    "        list_y_pred.append(cat_dict_inv[y])\n",
    "\n",
    "    \n",
    "    \n",
    "    # Make submission file\n",
    "    X_test['category'] = list_y_pred    \n",
    "    X = X_test.drop(columns=['title', 'language'])\n",
    "    X = X.set_index('id')\n",
    "    X.to_csv(os.path.join(PATH_SUB, 'sub_test07_mytk_prop10_lr{}_v{}.csv'.format(lr, v)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Vote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub07_mytk_06_v1 = pd.read_csv(os.path.join(PATH_SUB,'sub_test07_mytk_prop10_lr0.0006_v1.csv'))\n",
    "sub07_mytk_06_v2 = pd.read_csv(os.path.join(PATH_SUB,'sub_test07_mytk_prop10_lr0.0006_v2.csv'))\n",
    "sub07_mytk_06_v3 = pd.read_csv(os.path.join(PATH_SUB,'sub_test07_mytk_prop10_lr0.0006_v3.csv'))\n",
    "sub07_mytk_06_v4 = pd.read_csv(os.path.join(PATH_SUB,'sub_test07_mytk_prop10_lr0.0006_v4.csv'))\n",
    "sub07_mytk_06_v5 = pd.read_csv(os.path.join(PATH_SUB,'sub_test07_mytk_prop10_lr0.0006_v5.csv'))\n",
    "sub07_mytk_05_v1 = pd.read_csv(os.path.join(PATH_SUB,'sub_test07_mytk_prop10_lr0.0005_v1.csv'))\n",
    "sub07_mytk_05_v2 = pd.read_csv(os.path.join(PATH_SUB,'sub_test07_mytk_prop10_lr0.0005_v2.csv'))\n",
    "sub07_mytk_05_v3 = pd.read_csv(os.path.join(PATH_SUB,'sub_test07_mytk_prop10_lr0.0005_v3.csv'))\n",
    "sub07_mytk_05_v4 = pd.read_csv(os.path.join(PATH_SUB,'sub_test07_mytk_prop10_lr0.0005_v4.csv'))\n",
    "sub07_mytk_05_v5 = pd.read_csv(os.path.join(PATH_SUB,'sub_test07_mytk_prop10_lr0.0005_v5.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "\n",
    "list_counters = []\n",
    "# best 2\n",
    "models1 = [sub07_mytk_06_v4.category, sub07_mytk_06_v3.category ]\n",
    "models2 = [sub07_mytk_05_v3.category, sub07_mytk_05_v2.category ]\n",
    "# worst 3\n",
    "models3 = [sub07_mytk_06_v2.category, sub07_mytk_05_v1.category, sub07_mytk_06_v5.category ]\n",
    "models4 = [sub07_mytk_05_v1.category, sub07_mytk_05_v5.category, sub07_mytk_05_v4.category ]\n",
    "\n",
    "sumador1 = 0\n",
    "sumador2 = 0\n",
    "\n",
    "for i in range(len(sub08_03.category)):\n",
    "    cnt = Counter()\n",
    "    for model in models1:\n",
    "        cnt[model[i]] += 5\n",
    "    for model in models2:\n",
    "        cnt[model[i]] += 5\n",
    "    for model in models3:\n",
    "        cnt[model[i]] += 4\n",
    "    for model in models4:\n",
    "        cnt[model[i]] += 4\n",
    "    \n",
    "    list_counters.append(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate submission file of this group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "ide = []\n",
    "for i,count in enumerate(list_counters):\n",
    "   # print(max(count))\n",
    "    categories.append(max(count, key=count.get))\n",
    "    ide.append(i)\n",
    "\n",
    "sub_mix = pd.DataFrame(categories, columns = ['category']) #, index=['id])', 'b', 'c']) \n",
    "sub_mix['id'] = ide\n",
    "sub_mix = sub_mix.set_index('id')\n",
    "\n",
    "\n",
    "sub_mix.to_csv(os.path.join(PATH_SUB,'sub_mytk_07.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs231n",
   "language": "python",
   "name": "cs231n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
