{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.384412Z",
     "start_time": "2019-10-07T23:01:16.340452Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.410897Z",
     "start_time": "2019-10-07T23:01:18.385557Z"
    }
   },
   "outputs": [],
   "source": [
    "# path in which train.csv and test.csv are located\n",
    "\n",
    "minipath = os.getcwd()\n",
    "\n",
    "if not os.path.exists(os.path.join(minipath, 'data')):\n",
    "    os.mkdir(os.path.join(minipath, 'data'))\n",
    "    \n",
    "    \n",
    "PATH = os.path.join(minipath, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-10-07 21:20:29--  https://meli-data-challenge.s3.amazonaws.com/test.csv\n",
      "Resolving meli-data-challenge.s3.amazonaws.com (meli-data-challenge.s3.amazonaws.com)... 52.216.147.19\n",
      "Connecting to meli-data-challenge.s3.amazonaws.com (meli-data-challenge.s3.amazonaws.com)|52.216.147.19|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16467436 (16M) [text/csv]\n",
      "Saving to: ‘/home/michel/Documents/MachineLearning/kaggle/meli/github/data/test.csv’\n",
      "\n",
      "test.csv            100%[===================>]  15.70M  4.38MB/s    in 3.6s    \n",
      "\n",
      "2019-10-07 21:20:34 (4.38 MB/s) - ‘/home/michel/Documents/MachineLearning/kaggle/meli/github/data/test.csv’ saved [16467436/16467436]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://meli-data-challenge.s3.amazonaws.com/test.csv' -P $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-10-07 21:20:34--  https://meli-data-challenge.s3.amazonaws.com/train.csv.gz\n",
      "Resolving meli-data-challenge.s3.amazonaws.com (meli-data-challenge.s3.amazonaws.com)... 52.216.147.19\n",
      "Connecting to meli-data-challenge.s3.amazonaws.com (meli-data-challenge.s3.amazonaws.com)|52.216.147.19|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 691651384 (660M) [application/x-gzip]\n",
      "Saving to: ‘/home/michel/Documents/MachineLearning/kaggle/meli/github/data/train.csv.gz’\n",
      "\n",
      "train.csv.gz        100%[===================>] 659.61M  5.88MB/s    in 2m 5s   \n",
      "\n",
      "2019-10-07 21:22:40 (5.28 MB/s) - ‘/home/michel/Documents/MachineLearning/kaggle/meli/github/data/train.csv.gz’ saved [691651384/691651384]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://meli-data-challenge.s3.amazonaws.com/train.csv.gz' -P $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = os.path.join(PATH, 'train.csv.gz')\n",
    "\n",
    "!gunzip  $train_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make all the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.576910Z",
     "start_time": "2019-10-07T23:04:48.148Z"
    }
   },
   "outputs": [],
   "source": [
    "# make directories if don't exist\n",
    "\n",
    "\n",
    "if not os.path.exists(os.path.join(PATH, 'models')):\n",
    "    os.mkdir(os.path.join(PATH, 'models'))\n",
    "if not os.path.exists(os.path.join(PATH, 'vocabularies')):\n",
    "    os.mkdir(os.path.join(PATH, 'vocabularies'))\n",
    "if not os.path.exists(os.path.join(PATH, 'submissions')):\n",
    "    os.mkdir(os.path.join(PATH, 'submissions'))\n",
    "if not os.path.exists(os.path.join(PATH, 'datasets')):\n",
    "    os.mkdir(os.path.join(PATH, 'datasets'))\n",
    "if not os.path.exists(os.path.join(PATH, 'stats')):\n",
    "    os.mkdir(os.path.join(PATH, 'stats'))\n",
    "\n",
    "    \n",
    "PATH_DATASET = os.path.join(PATH, 'datasets')\n",
    "\n",
    "PATH_VOCAB = os.path.join(PATH, 'vocabularies')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.577580Z",
     "start_time": "2019-10-07T23:04:48.151Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "path = '/home/gartner/Documents/meli/github/data'\n",
    "\n",
    "file = os.path.join(PATH, 'train.csv')\n",
    "train = pd.read_csv(file)\n",
    "\n",
    "file = os.path.join(PATH, 'test.csv')\n",
    "test = pd.read_csv(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make classes dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.578135Z",
     "start_time": "2019-10-07T23:04:48.154Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#subset = pd.read_hdf('train.h5')\n",
    "subset = train.drop(columns=['language', 'label_quality'])\n",
    "\n",
    "cat_dict = {}\n",
    "for i,category in enumerate(subset.category.unique()):\n",
    "    cat_dict[category] = i\n",
    "    \n",
    "cat_dict_inv = []\n",
    "for i,category in enumerate(subset.category.unique()):\n",
    "    cat_dict_inv.append(category)\n",
    "    \n",
    "counts = subset.groupby('category').count()\n",
    "\n",
    "values = counts.values\n",
    "categories = counts.index\n",
    "\n",
    "dict_count = {}\n",
    "for value,category in zip(values,categories):\n",
    "    dict_count[cat_dict[category]] = value[0]\n",
    "    \n",
    "    \n",
    "classes_dict = {}\n",
    "\n",
    "classes_dict['class_stoi'] = cat_dict\n",
    "classes_dict['class_itos'] = cat_dict_inv\n",
    "classes_dict['class_count'] = dict_count\n",
    "\n",
    "with open(os.path.join(PATH,'classes_dict.pkl'), 'wb') as f:\n",
    "    pickle.dump(classes_dict, f)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make classes weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.578843Z",
     "start_time": "2019-10-07T23:04:48.158Z"
    }
   },
   "outputs": [],
   "source": [
    "train_unrel = train[train.label_quality == 'unreliable']\n",
    "train_rel = train[train.label_quality == 'reliable']\n",
    "\n",
    "\n",
    "class_rel = list(train_rel.category.unique())\n",
    "class_unrel = list(train_unrel.category.unique())\n",
    "\n",
    "list_train_unrel = list(train_unrel.groupby('category'))\n",
    "list_train_rel = list(train_rel.groupby('category'))\n",
    "\n",
    "a = train_rel.groupby('category').count().title\n",
    "b = train_unrel.groupby('category').count().title\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.579430Z",
     "start_time": "2019-10-07T23:04:48.160Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "len(cat_dict_inv), cat_dict[a.index[1000]], cat_dict[b.index[1000]]\n",
    "\n",
    "weights = np.zeros((len(cat_dict_inv), 3))\n",
    "\n",
    "    \n",
    "for i in range(len(a)):\n",
    "    # Category\n",
    "    cat = a.index[i]\n",
    "    # idx in convention\n",
    "    idx = cat_dict[cat]\n",
    "    # Count\n",
    "    count = a[i]\n",
    "    \n",
    "    weights[idx,0] = count\n",
    "    \n",
    "for i in range(len(b)):\n",
    "    # Category\n",
    "    cat = b.index[i]\n",
    "    # idx in convention\n",
    "    idx = cat_dict[cat]\n",
    "    # Count\n",
    "    count = b[i]\n",
    "    \n",
    "    weights[idx,1] = count\n",
    "    \n",
    "for i in range(len(cat_dict_inv)):\n",
    "    weights[i,2] = weights[i,0] / weights[i,1] \n",
    "\n",
    "    \n",
    "with open( os.path.join(PATH, 'classes_weights.pkl'), 'wb') as f:\n",
    "    pickle.dump(weights, f)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make vocabularies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make vocabulary using training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.580029Z",
     "start_time": "2019-10-07T23:04:48.163Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Load submission test set\n",
    "train = pd.read_csv( os.path.join(PATH,'train.csv') )\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    min_df=1\n",
    "    )\n",
    "\n",
    "\n",
    "vector_cat = vectorizer.fit_transform(train.title)\n",
    "\n",
    "    \n",
    "vocab = vectorizer.vocabulary_\n",
    "with open( os.path.join(PATH_VOCAB,'vocab_all.pkl'),'wb') as f:\n",
    "    pickle.dump(vocab, f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make vocabulary using test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.580612Z",
     "start_time": "2019-10-07T23:04:48.166Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "# Load submission test set\n",
    "test = pd.read_csv( os.path.join(PATH,'test.csv') )\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    min_df=1\n",
    "    )\n",
    "\n",
    "\n",
    "vector_cat = vectorizer.fit_transform(test.title)\n",
    "\n",
    "    \n",
    "vocab = vectorizer.vocabulary_\n",
    "with open(os.path.join(PATH_VOCAB, 'vocab_test.pkl'),'wb') as f:\n",
    "    pickle.dump(vocab, f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vocabulary with similar spelling in test set\n",
    "\n",
    "We assign all words with similar spelling to the same integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.581184Z",
     "start_time": "2019-10-07T23:04:48.169Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load vocabulary\n",
    "file = open(os.path.join(PATH_VOCAB, 'vocab_test.pkl'), 'rb')\n",
    "vocab = pickle.load(file)\n",
    "\n",
    "# Make a list with the vocab\n",
    "list_vocab = []\n",
    "\n",
    "for key in vocab:\n",
    "    list_vocab.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word we look for words in the list that are spelled similar. Then we assign all those words to the same integer and remove all of them from the list.\n",
    "\n",
    "This preprocessing requires few hours in my cpu..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.581765Z",
     "start_time": "2019-10-07T23:04:48.173Z"
    }
   },
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "import time\n",
    "\n",
    "new_vocab = {}\n",
    "\n",
    "# Itero por todos los elementos de list_vocab en ppio\n",
    "n = 0\n",
    "start = time.time()\n",
    "for i in range( len(list_vocab)):\n",
    "\n",
    "    # Busco matches\n",
    "    matches = get_close_matches(list_vocab[-1], list_vocab, n=20,  cutoff=0.9)\n",
    "    \n",
    "    \n",
    "    if n%100==0:\n",
    "        print(matches, time.time()-start)\n",
    "    \n",
    "    # En todas las palabras \n",
    "    for word in matches:\n",
    "        new_vocab[word] = i\n",
    "        \n",
    "        # Remove words that we've already used\n",
    "        list_vocab.remove(word)\n",
    "        n+=1\n",
    "        \n",
    "    # We stop when we use all words\n",
    "    if n==len(vocab):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.582296Z",
     "start_time": "2019-10-07T23:04:48.177Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save \n",
    "\n",
    "with open(os.path.join(PATH_VOCAB, 'vocab_test_red.pkl'), 'wb') as f:\n",
    "    pickle.dump(new_vocab, f)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make vocabulary in which each class is greater than 0.0015\n",
    "\n",
    "for each class, we only keep tokens that appears in more than 0.0015 of the total examples from that class. Takes about 20 minutes in my cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.582933Z",
     "start_time": "2019-10-07T23:04:48.180Z"
    }
   },
   "outputs": [],
   "source": [
    "def most_common(vectors, vocab_itos, cant, umbral):\n",
    "    list_words=[]\n",
    "    b = vectors.sum(axis=0)\n",
    "    c = b[b>1]\n",
    "    more_common = c.argsort()[-3:][::-1]\n",
    "    n = more_common.size\n",
    "    for value in range(cant):\n",
    "        valor = more_common[0, n-1-value]\n",
    "        #print(value, valor, vocab_itos[valor], b[0, valor])\n",
    "        \n",
    "        if (b[0, valor] > max(1,umbral)):\n",
    "            list_words.append(vocab_itos[valor])\n",
    "    return list_words\n",
    "\n",
    "    categories = subset.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.583544Z",
     "start_time": "2019-10-07T23:04:48.186Z"
    }
   },
   "outputs": [],
   "source": [
    "fraccion=0.0015\n",
    "    \n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    strip_accents='unicode',\n",
    "    min_df=2\n",
    "    )\n",
    "\n",
    "list_list_words = []\n",
    "\n",
    "for i,category in enumerate(categories):\n",
    "    #print('Leo set: {}'.format(category))\n",
    "    subset_cat = subset[subset.category==category]\n",
    "\n",
    "\n",
    "    vector_cat = vectorizer.fit_transform(subset_cat.title)\n",
    "    vocab_stoi_cat = vectorizer.vocabulary_\n",
    "    vocab_itos_cat =  {v: k for k, v in vocab_stoi_cat.items()}\n",
    "\n",
    "    umbral = fraccion * subset_cat.shape[0]\n",
    "    cant = len(vocab_stoi_cat)\n",
    "    list_words = most_common(vector_cat, vocab_itos_cat, cant, umbral)\n",
    "\n",
    "    print(i, category, subset_cat.shape, len(vocab_stoi_cat), len(list_words))\n",
    "\n",
    "    list_list_words.append(list_words)\n",
    "\n",
    "    list_vocab = set().union(*list_list_words)\n",
    "\n",
    "\n",
    "vocab = {}\n",
    "for i, word in enumerate(list_vocab):\n",
    "    vocab[word] = i\n",
    "\n",
    "with open(os.path.join(PATH_VOCAB, 'vocab_0.0015.pkl'), 'wb') as f:\n",
    "        pickle.dump(vocab, f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.584189Z",
     "start_time": "2019-10-07T23:04:48.190Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(PATH,'train.csv'))\n",
    "\n",
    "file = open(os.path.join(PATH_VOCAB, 'vocab_test.pkl'), 'rb')\n",
    "vocab_test = pickle.load(file)\n",
    "\n",
    "file = open(os.path.join(PATH_VOCAB, 'vocab_all.pkl'), 'rb')\n",
    "vocab_all = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter total database\n",
    "\n",
    "We tokenize each example of the training set using 'vocab_all' and 'vocab_test'. \n",
    "\n",
    "We remove all the examples in which the number of the tokens of the title generated using vocab_test is less than (some threshold) times the number of tokens of the title generated using vocab_all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.584723Z",
     "start_time": "2019-10-07T23:04:48.194Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer1 = CountVectorizer(analyzer = 'word',\n",
    "                             strip_accents='unicode',\n",
    "                             vocabulary=vocab_test\n",
    "                            )\n",
    "\n",
    "tokenizer1 = vectorizer1.build_analyzer()\n",
    "\n",
    "vectorizer2 = CountVectorizer(analyzer = 'word',\n",
    "                             strip_accents='unicode',\n",
    "                             vocabulary=vocab_all\n",
    "                            )\n",
    "\n",
    "tokenizer2 = vectorizer2.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.585358Z",
     "start_time": "2019-10-07T23:04:48.197Z"
    }
   },
   "outputs": [],
   "source": [
    "vectors1 = vectorizer1.fit_transform(train.title)\n",
    "vectors2 = vectorizer2.fit_transform(train.title)\n",
    "\n",
    "sumas1 = vectors1.sum(axis=1)\n",
    "sumas2 = vectors2.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.585946Z",
     "start_time": "2019-10-07T23:04:48.200Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "numbers = np.zeros((vectors1.shape[0], 2))\n",
    "list_coc = []\n",
    "\n",
    "for i in range(vectors1.shape[0]):\n",
    "    numbers[i,0] = sumas1[i,0]\n",
    "    numbers[i,1] = sumas2[i,0]\n",
    "    if numbers[i,1] !=0:\n",
    "        list_coc.append(numbers[i,0] / numbers[i,1])\n",
    "    else:\n",
    "        list_coc.append(1.2)\n",
    "    \n",
    "umbral1 = 0.9\n",
    "umbral2 = 0.8\n",
    "umbral3 = 0.7\n",
    "umbral4 = 0.6\n",
    "umbral5 = 0.5\n",
    "\n",
    "prior = [i for i in range(len(list_coc)) if list_coc[i]==1]\n",
    "prior1 = [i for i in range(len(list_coc)) if umbral1 <= list_coc[i] < 1]\n",
    "prior2 = [i for i in range(len(list_coc)) if umbral2 <= list_coc[i] < umbral1]\n",
    "prior3 = [i for i in range(len(list_coc)) if umbral3 <= list_coc[i] < umbral2]\n",
    "prior4 = [i for i in range(len(list_coc)) if umbral4 <= list_coc[i] < umbral3]\n",
    "prior5 = [i for i in range(len(list_coc)) if umbral5 <= list_coc[i] < umbral4]\n",
    "prior6 = [i for i in range(len(list_coc)) if list_coc[i] < umbral5]\n",
    "prior7 = [i for i in range(len(list_coc)) if list_coc[i] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.586512Z",
     "start_time": "2019-10-07T23:04:48.203Z"
    }
   },
   "outputs": [],
   "source": [
    "len(prior), len(prior1), len(prior2), len(prior3), len(prior4), len(prior5), len(prior6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make 4 subsets from the train set, including examples with coincidence greater than 0.7, 0.8, 0.9 and equal to 1 respectively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.587105Z",
     "start_time": "2019-10-07T23:04:48.207Z"
    }
   },
   "outputs": [],
   "source": [
    "train_07 = train.iloc[prior+prior1+prior2+prior3 , :]\n",
    "train_08 = train.iloc[prior+prior1+prior2 , :]\n",
    "train_09 = train.iloc[prior+prior1, :]\n",
    "train_10 = train.iloc[prior, :]\n",
    "\n",
    "list_train = [train, train_07, train_08, train_09, train_10]\n",
    "names = ['', '_07', '_08', '_09', '_10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T23:01:18.587677Z",
     "start_time": "2019-10-07T23:04:48.211Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "for train_set, name in zip(list_train, names):\n",
    "\n",
    "    X = shuffle(train_set)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test = train_test_split(X, test_size=0.025, stratify=X.category, random_state=3)\n",
    "\n",
    "    # Reset indexes\n",
    "    X_train = X_train.reset_index()\n",
    "    X_test = X_test.reset_index()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #split data in reliable and unreliable\n",
    "    train_unrel = X_train[X_train.label_quality == 'unreliable']\n",
    "    train_rel = X_train[X_train.label_quality == 'reliable']\n",
    "\n",
    "\n",
    "    class_rel = list(train_rel.category.unique())\n",
    "    class_unrel = list(train_unrel.category.unique())\n",
    "\n",
    "    X_train = X_train.drop(columns=['label_quality', 'language', 'index'])\n",
    "    X_test = X_test.drop(columns=['label_quality', 'language', 'index'])\n",
    "    train_unrel = train_unrel.drop(columns=['label_quality', 'language', 'index'])\n",
    "    train_rel = train_rel.drop(columns=['label_quality', 'language', 'index'])\n",
    "\n",
    "    # Save\n",
    "    X_train.to_hdf(os.path.join(PATH_DATASET,'train_set{}.h5'.format(name)), key='X_train', mode='w')\n",
    "    X_test.to_hdf(os.path.join(PATH_DATASET,'test_set{}.h5'.format(name)), key='X_test', mode='w')\n",
    "\n",
    "    train_unrel.to_hdf(os.path.join(PATH_DATASET,'train_set_unrel{}.h5'.format(name)), key='train_unrel', mode='w')\n",
    "    train_rel.to_hdf(os.path.join(PATH_DATASET,'train_set_rel{}.h5'.format(name)), key='train_rel', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs231n",
   "language": "python",
   "name": "cs231n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
