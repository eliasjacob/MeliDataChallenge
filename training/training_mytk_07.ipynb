{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T17:24:08.203059Z",
     "start_time": "2019-10-07T17:24:04.606349Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we call 'test' the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T17:24:08.207884Z",
     "start_time": "2019-10-07T17:24:08.205203Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# get data PATH from current directory\n",
    "PATH = os.path.join(Path(os.getcwd()).parent, 'data')\n",
    "\n",
    "\n",
    "PATH_DATASET = os.path.join(PATH, 'datasets')\n",
    "\n",
    "PATH_VOCAB = os.path.join(PATH, 'vocabularies')\n",
    "\n",
    "PATH_MODEL = os.path.join(PATH, 'models')\n",
    "\n",
    "PATH_STATS = os.path.join(PATH, 'stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T17:24:57.918462Z",
     "start_time": "2019-10-07T17:24:08.209485Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load training and test set in pandas dataframe\n",
    "\n",
    "train = pd.read_hdf(os.path.join(PATH_DATASET,'train_set_07.h5'))\n",
    "test = pd.read_hdf(os.path.join(PATH_DATASET,'test_set_07.h5'))\n",
    "\n",
    "\n",
    "train_rel = pd.read_hdf(os.path.join(PATH_DATASET,'train_set_rel_07.h5'))\n",
    "train_unrel = pd.read_hdf(os.path.join(PATH_DATASET,'train_set_unrel_07.h5'))\n",
    "\n",
    "# Load vocabulary\n",
    "\n",
    "file = open(os.path.join(PATH_VOCAB,'vocab_test.pkl'), 'rb')\n",
    "vocab = pickle.load(file)\n",
    "\n",
    "\n",
    "# Load classes dictionary\n",
    "\n",
    "file = open(os.path.join(PATH,'classes_dict.pkl'), 'rb')\n",
    "classes_dict = pickle.load(file)\n",
    "\n",
    "cat_dict = classes_dict['class_stoi']\n",
    "cat_dict_inv = classes_dict['class_itos']\n",
    "dict_count = classes_dict['class_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define my tokenizer\n",
    "\n",
    "In this model we  will use a Bag of words approach using sk-learn CountVectorizer. We define our own tokenizer.\n",
    "\n",
    "Originally we have our main vocabulary that represents a biyective function mapping one token with one number. Using that dictionary we have made a new dictionary that maps each group of words that are spelled similarly to the same integer.\n",
    "\n",
    "\n",
    "for example: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T17:24:58.257830Z",
     "start_time": "2019-10-07T17:24:57.950136Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "file = open(os.path.join(PATH_VOCAB,'vocab_test_red.pkl'), 'rb')\n",
    "vocab_mix = pickle.load(file)\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                             strip_accents='unicode',\n",
    "                             min_df=2\n",
    "                            )\n",
    "\n",
    "tokenizer = vectorizer.build_analyzer()\n",
    "\n",
    "\n",
    "counter = Counter(vocab_mix.values())\n",
    "\n",
    "a = [x for x in counter if counter[x]>1]\n",
    "\n",
    "\n",
    "\n",
    "vocab_transf = {}\n",
    "for key in vocab_mix:\n",
    "    vocab_transf[vocab_mix[key]] = 'r_{}'.format(key)\n",
    "    \n",
    "vocab_transf\n",
    "\n",
    "\n",
    "def my_tokenizer(string):\n",
    "    tokens = tokenizer(string)\n",
    "    list_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in vocab_mix:\n",
    "            num = vocab_mix[token]\n",
    "            #print(token, vocab_mix[token], counter[num])\n",
    "            if counter[num]>1:\n",
    "                list_tokens.append(vocab_transf[vocab_mix[token]])\n",
    "            list_tokens.append(token)\n",
    "\n",
    "    #print(list_tokens, string)\n",
    "    return list_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize\n",
    "\n",
    "Make BoW using my tokenizer. We should notice that we don't put explicitly a 'vocabulary' in the CountVectorizer object. However, the vocabulary is implicitly restricted by the tokenizer we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T17:30:01.413863Z",
     "start_time": "2019-10-07T17:24:58.259114Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the vectorizer to generate the BoW\n",
    "\n",
    "vectorizer_tok = CountVectorizer(\n",
    "                             analyzer = 'word',\n",
    "                             strip_accents='unicode',\n",
    "                             tokenizer=my_tokenizer\n",
    "                            )\n",
    "\n",
    "\n",
    "# Reset indexes of the training set before vectorizer\n",
    "train = train.reset_index()\n",
    "\n",
    "# Vectorize the training and test_set\n",
    "vectors_train = vectorizer_tok.fit_transform(train.title)\n",
    "\n",
    "vectors_test = vectorizer_tok.transform(test.title)\n",
    "\n",
    "\n",
    "y_train = train.category.values\n",
    "\n",
    "X_test = vectors_test\n",
    "y_test = test.category.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the vocabulary output of the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T17:30:01.694604Z",
     "start_time": "2019-10-07T17:30:01.414924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19500000, 122573)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = vectorizer_tok.vocabulary_\n",
    "\n",
    "with open(os.path.join(PATH_VOCAB,'vocab_07_test_mytk.pkl'), 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "    \n",
    "vectors_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util functions\n",
    "\n",
    "We define some util functions\n",
    "\n",
    "- Functions to sample in a balanced way and priortizing reliable examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T17:30:19.757622Z",
     "start_time": "2019-10-07T17:30:01.695771Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split training set by categories\n",
    "list_train_rel = list(train_rel.groupby('category'))\n",
    "list_train_unrel = list(train_unrel.groupby('category'))\n",
    "\n",
    "class_rel = train_rel.groupby('category').count().title.index\n",
    "class_unrel = train_unrel.groupby('category').count().title.index\n",
    "\n",
    "\n",
    "file = open(os.path.join(PATH,'classes_weights.pkl'), 'rb')\n",
    "weights = pickle.load(file)\n",
    "\n",
    "\n",
    "def numero(weight, num_examples, proporcion):\n",
    "    \n",
    "    if weight==0:\n",
    "        return 0\n",
    "        \n",
    "    return int(min(max(1, round(num_examples*weight*proporcion)), num_examples-1))\n",
    "\n",
    "\n",
    "def index_sample(list_train_rel, list_train_unrel, num_samples, proporcion):\n",
    "    list_idx = []\n",
    "    \n",
    "    # Unreliable indexes\n",
    "    \n",
    "    for i in range(len(class_unrel)):\n",
    "        # Convention index\n",
    "        idx = cat_dict[class_unrel[i]]\n",
    "        number = num_samples - numero(weights[idx,2], num_samples, proporcion)\n",
    "        list_idx += list(np.random.choice(list_train_unrel[i][1].index, size=number))\n",
    "    \n",
    "    # Reliable indexes\n",
    "    for i,category in enumerate(class_rel):\n",
    "        \n",
    "        idx = cat_dict[class_rel[i]]\n",
    "        number = numero(weights[idx,2], num_samples, proporcion)\n",
    "        list_idx += list(np.random.choice(list_train_rel[i][1].index, size=number))\n",
    "    \n",
    "    \n",
    "    return list_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions in CPU\n",
    "\n",
    "Functions to calculate bacc and accuracy using CPU.\n",
    "\n",
    "We train the model using GPU, but we don't have enough memory to run the predictions in the validation set while training. So that, at the end of each epoch we set the model to the cpu, make predictions in the validation set and then set the model to gpu and continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T17:32:54.861030Z",
     "start_time": "2019-10-07T17:32:54.853407Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "from numpy import array\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "\n",
    "\n",
    "def report(model, X_test, y_test):\n",
    "    \n",
    "    x_tensor = csr_to_tensor(X_test)\n",
    "    model_cpu = model.to('cpu')\n",
    "    outputs = model_cpu(x_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    predicted.cpu().numpy()\n",
    "    y_label = []\n",
    "    for val in y_test:\n",
    "        y_label.append(cat_dict[val])\n",
    "    y_label = np.array(y_label)\n",
    "    \n",
    "    return balanced_accuracy_score(y_label, predicted), accuracy_score(y_label, predicted)\n",
    "\n",
    "\n",
    "def csr_to_tensor(X_sample):\n",
    "    coo = coo_matrix(X_sample)\n",
    "\n",
    "    values = coo.data\n",
    "    indices = np.vstack((coo.row, coo.col))\n",
    "\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    shape = coo.shape\n",
    "\n",
    "    return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "\n",
    "def numerate(y):\n",
    "    index = []\n",
    "    for i in y:\n",
    "        index.append(cat_dict[i])\n",
    "    return np.array(index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T17:35:06.051959Z",
     "start_time": "2019-10-07T17:32:55.669274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [100/10], Loss: 1.5869\n",
      "Epoch [1/50], Step [200/10], Loss: 0.9928\n",
      "Epoch [1/50], Step [300/10], Loss: 0.8523\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-cfc4bcd86d1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mX_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mx_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_size = vectors_train.shape[1]\n",
    "num_classes = len(train.category.unique())\n",
    "H = num_classes\n",
    "\n",
    "H1 = round(1.6*num_classes)\n",
    "\n",
    "vec_proporcion = [10, 10]\n",
    "vec_num_epochs = [50, 50] \n",
    "#batch_size = 128\n",
    "vec_learning_rate = [0.0006, 0.0005]\n",
    "\n",
    "total_step = 10\n",
    "\n",
    "for learning_rate, num_epochs, prop in zip(vec_learning_rate, vec_num_epochs, vec_proporcion):\n",
    "\n",
    "    ## Define model\n",
    "    model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(input_size, H1),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Dropout(p=0.75),\n",
    "          torch.nn.Linear(H1, num_classes),\n",
    "    ).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Some training stats\n",
    "    model_stats = {}\n",
    "    model_stats['epoch'] = []\n",
    "    model_stats['avg_loss'] = []\n",
    "    model_stats['accuracy'] = []\n",
    "    model_stats['bacc'] = []\n",
    "\n",
    "    # Record maximum BACC value\n",
    "    max_bacc = 0\n",
    "    num_max = 0\n",
    "    \n",
    "    \n",
    "    # Epochs aren't 'real epochs' in which we use all the training set. \n",
    "    # Instead each epochs is a collection of 1000 mini-batches generated using the sampling defined above\n",
    "    # Each epoch only defines when and how many times we evaluate the model using the validation set.\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        list_loss = []\n",
    "        \n",
    "        for i in range(1000):\n",
    "            \n",
    "            # List of index of the minibatch using the function defined above\n",
    "            list_idx = index_sample(list_train_rel, list_train_unrel, 10, prop)\n",
    "            \n",
    "            X = vectors_train[list_idx, :]\n",
    "            y = y_train[list_idx]\n",
    "            \n",
    "            X_sample, y_sample = shuffle(X, y)\n",
    "\n",
    "            x_tensor = csr_to_tensor(X_sample).to(device)\n",
    "\n",
    "            labels = torch.from_numpy(numerate(y_sample)).long().to(device)\n",
    "\n",
    "            outputs = model(x_tensor)\n",
    "            #images.to('cpu')\n",
    "            #torch.cuda.empty_cache()\n",
    "            \n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            lo = loss.item()\n",
    "            list_loss.append(lo)\n",
    "            # Test\n",
    "            #outputs.to('cpu')\n",
    "            #torch.cuda.empty_cache()\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch+1, num_epochs, i+1, total_step, lo )) ##ss.item()))\n",
    "\n",
    "        ## Statistics\n",
    "\n",
    "        # Bacc and accuracy\n",
    "        \n",
    "        bacc, accuracy = report(model.eval(), X_test, y_test)\n",
    "        print(bacc,accuracy)\n",
    "        model.to(device)\n",
    "\n",
    "        # Save in the model_stats dictionary\n",
    "        model_stats['epoch'].append(epoch)\n",
    "        model_stats['bacc'].append(bacc)\n",
    "        model_stats['accuracy'].append(accuracy)\n",
    "        model_stats['avg_loss'].append(np.array(list_loss).mean())\n",
    "        \n",
    "        # We upgrade the model each time bacc > max_bacc\n",
    "        # Also,  we store the model of the last 5 upgrades\n",
    "        \n",
    "        if bacc > max_bacc:\n",
    "            num_max += 1\n",
    "            max_bacc = bacc\n",
    "            torch.save(model, os.path.join(PATH_MODEL,'model_test07_mytk_prop{}_lr{}_v{}.pt'.format(prop, learning_rate, num_max%5+1)))\n",
    "\n",
    "\n",
    "        with open(os.path.join(PATH_STATS,'stats_test07_mytk_prop{}_lr{}.pt'.format(prop, learning_rate)), 'wb') as f:\n",
    "            pickle.dump(model_stats, f)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs231n",
   "language": "python",
   "name": "cs231n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
